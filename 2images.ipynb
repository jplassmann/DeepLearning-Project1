{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import models\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes  = prologue.generate_pair_sets(1000)\n",
    "train_target.unsqueeze_(1); test_target.unsqueeze_(1)\n",
    "print('train_input', train_input.size(), 'train_target', train_target.size(), 'train_classes', train_classes.size())\n",
    "print('test_input', test_input.size(), 'test_target', test_target.size(), 'test_classes', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model, simplest one\n",
    "net1 = models.Net1()\n",
    "# Second model introduces weight sharing for the convolutional layer\n",
    "net2 = models.Net2()\n",
    "# Third model, we use the label of the digits as an auxiliary loss\n",
    "net3 = models.Net3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target,batch_size=50):\n",
    "    nb_data_errors = 0\n",
    "    \n",
    "    for inputs, targets in zip(data_input.split(batch_size), data_target.split(batch_size)):\n",
    "        output = model(inputs)\n",
    "        output = output.narrow(dim=1,start=0,length=1)\n",
    "        output = torch.ge(output,0.5).float()\n",
    "        for k in range(len(targets)):\n",
    "            if output[k] != targets[k]:\n",
    "                nb_data_errors += 1\n",
    "                \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, test_input, test_target,\n",
    "                train_classes=None, use_auxiliary_losses=False,\n",
    "                round=0, epochs=25,eta=0.4,batch_size=100):\n",
    "    \n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    \n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mu).div_(std)\n",
    "    test_input.sub_(mu).div_(std)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for inputs, targets in zip(train_input.split(batch_size), train_target.split(batch_size)):\n",
    "            output = model(inputs) \n",
    "            loss = criterion(output, targets.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "        test_accuracy = compute_nb_errors(model, test_input, test_target)\n",
    "        train_accuracy = compute_nb_errors(model, train_input, train_target)\n",
    "        test_accuracy = 100 * (1 - test_accuracy / test_input.size(0))\n",
    "        train_accuracy = 100 * (1 - train_accuracy / train_input.size(0))\n",
    "        if (round==0):\n",
    "            print(f\"Epoch # {i+1} / train accuracy: {train_accuracy:.2f} / test accuracy: {test_accuracy:.2f}\")\n",
    "        \n",
    "    if (round>0):\n",
    "        print(f\"Round # {round} / train accuracy: {train_accuracy:.2f} / test accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    return test_accuracy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_auxiliary_loss(model, train_input, train_target, test_input, test_target, train_classes,\n",
    "                round=0, epochs=25,eta=0.4,batch_size=100):\n",
    "    \n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    auxiliary_criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    \n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mu).div_(std)\n",
    "    test_input.sub_(mu).div_(std)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # model.train(True)\n",
    "        for inputs, targets, class_targets in zip(train_input.split(batch_size),\n",
    "                                   train_target.split(batch_size),\n",
    "                                   train_classes.split(batch_size)):\n",
    "            output = model(inputs) \n",
    "            # Prediction of which digit is larger\n",
    "            l_1 = criterion(output.narrow(dim=1,start=0,length=1), targets.float())\n",
    "            # Auxiliary losses for prediciting the actual digits\n",
    "            l_2 = auxiliary_criterion(output.narrow(dim=1,start=1,length=10),\n",
    "                                      class_targets[:,0])\n",
    "            l_3 = auxiliary_criterion(output.narrow(dim=1,start=11,length=10),\n",
    "                                      class_targets[:,1])\n",
    "            loss = l_1 + l_2 + l_3\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "\n",
    "        test_accuracy = compute_nb_errors(model, test_input, test_target)\n",
    "        train_accuracy = compute_nb_errors(model, train_input, train_target)\n",
    "        test_accuracy = 100 * (1 - test_accuracy / test_input.size(0))\n",
    "        train_accuracy = 100 * (1 - train_accuracy / train_input.size(0))\n",
    "        if (round==0):\n",
    "            print(f\"Epoch # {i+1} / train accuracy: {train_accuracy:.2f} / test accuracy: {test_accuracy:.2f}\")\n",
    "        \n",
    "    if (round>0):\n",
    "        print(f\"Round # {round} / train accuracy: {train_accuracy:.2f} / test accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    return test_accuracy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Round Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = models.Net1()\n",
    "_ = train_model(net1, train_input, train_target, test_input, test_target, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = models.Net2()\n",
    "_ = train_model(net2, train_input, train_target, test_input, test_target, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net3 = models.Net3()\n",
    "_ = train_model_auxiliary_loss(net3, train_input, train_target, test_input, test_target, train_classes,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4 = models.Net4()\n",
    "_ = train_model_auxiliary_loss(net3, train_input, train_target, test_input, test_target, train_classes,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Round Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_round = 100\n",
    "epochs_per_round = 60\n",
    "test_accuracy_1 = [0] * nb_round\n",
    "test_accuracy_2 = [0] * nb_round\n",
    "test_accuracy_3 = [0] * nb_round\n",
    "test_accuracy_4 = [0] * nb_round\n",
    "\n",
    "print(\"*** Testing Model 1 ***\")\n",
    "for i in range(0, nb_round):\n",
    "    t0 = time.perf_counter()\n",
    "    net1 = models.Net1()\n",
    "    test_accuracy_1[i] = train_model(net1, train_input, train_target, test_input,\n",
    "                                     test_target, round=i+1,epochs=epochs_per_round)\n",
    "t_tot = time.perf_counter() - t0\n",
    "print(f\"Mean : {mean(test_accuracy_1):.2f} / STD : {stdev(test_accuracy_1):.2f} / Total Time : {t_tot:.2f} / Mean Time : {t_tot/nb_round:.2f}\")\n",
    "\n",
    "print(\"*** Testing Model 2 ***\")\n",
    "for i in range(0, nb_round):\n",
    "    t0 = time.perf_counter()\n",
    "    net2 = models.Net2()\n",
    "    test_accuracy_2[i] = train_model(net2, train_input, train_target, test_input,\n",
    "                                     test_target, round=i+1,epochs=epochs_per_round)\n",
    "t_tot = time.perf_counter() - t0\n",
    "print(f\"Mean : {mean(test_accuracy_2):.2f} / STD : {stdev(test_accuracy_2):.2f} / Total Time : {t_tot:.2f} / Mean Time : {t_tot/nb_round:.2f}\")\n",
    "\n",
    "print(\"*** Testing Model 3 ***\")\n",
    "for i in range(0, nb_round):\n",
    "    t0 = time.perf_counter()\n",
    "    net3 = models.Net3()\n",
    "    test_accuracy_3[i] = train_model_auxiliary_loss(net3, train_input, train_target, test_input,\n",
    "                                                    test_target, train_classes,\n",
    "                                                    round=i+1,epochs=epochs_per_round)\n",
    "t_tot = time.perf_counter() - t0\n",
    "print(f\"Mean : {mean(test_accuracy_3):.2f} / STD : {stdev(test_accuracy_3):.2f} / Total Time : {t_tot:.2f} / Mean Time : {t_tot/nb_round:.2f}\")\n",
    "\n",
    "print(\"*** Testing Model 4 ***\")\n",
    "for i in range(0, nb_round):\n",
    "    t0 = time.perf_counter()\n",
    "    net4 = models.Net4()\n",
    "    test_accuracy_4[i] = train_model_auxiliary_loss(net4, train_input, train_target, test_input,\n",
    "                                                    test_target, train_classes,\n",
    "                                                    round=i+1,epochs=epochs_per_round)\n",
    "t_tot = time.perf_counter() - t0\n",
    "print(f\"Mean : {mean(test_accuracy_4):.2f} / STD : {stdev(test_accuracy_4):.2f} / Total Time : {t_tot:.2f} / Mean Time : {t_tot/nb_round:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean : {mean(test_accuracy_4):.2f} / STD : {stdev(test_accuracy_4):.2f} / Total Time : {t_tot:.2f} / Mean Time : {t_tot/nb_round:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = net(test_input[0:20,0].reshape(20,1,14,14), test_input[0:20,1].reshape(20,1,14,14))\n",
    "res[res>0.5] = 1\n",
    "res[res <= 0.5] = 0\n",
    "for i in range(0, 20):\n",
    "    plt.subplot(121),plt.imshow(test_input[i, 0].view(14,14)),plt.title('Original')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(test_input[i, 1].view(14,14)),plt.title('gradient')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    print(res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.overleaf.com/6688321767qwjpzsgnrdqb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
